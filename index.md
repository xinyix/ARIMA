## Introduction


### Generate Data
We first load and print our chosen time series data
```
## load in data
> data(AirPassengers)

## print data
> AirPassengers
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
1949 112 118 132 129 121 135 148 148 136 119 104 118
1950 115 126 141 135 125 149 170 170 158 133 114 140
1951 145 150 178 163 172 178 199 199 184 162 146 166
1952 171 180 193 181 183 218 230 242 209 191 172 194
1953 196 196 236 235 229 243 264 272 237 211 180 201
1954 204 188 235 227 234 264 302 293 259 229 203 229
1955 242 233 267 269 270 315 364 347 312 274 237 278
1956 284 277 317 313 318 374 413 405 355 306 271 306
1957 315 301 356 348 355 422 465 467 404 347 305 336
1958 340 318 362 348 363 435 491 505 404 359 310 337
1959 360 342 406 396 420 472 548 559 463 407 362 405
1960 417 391 419 461 472 535 622 606 508 461 390 432
```

### Make Inferences
Look at some basic behaviour of the series
```
## make sure its a time series object
> class(AirPassengers)
[1] "ts"

## get its distribution
> summary(AirPassengers)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  104.0   180.0   265.5   280.3   360.5   622.0 

## plot the series
> plot(AirPassengers)
```
<p style='text-align:center'><img src='https://github.com/xinyix/ARIMA/blob/master/data.jpg?raw=true'></p>

 Now we sketch out the general trend by aggregating the series according to years
 ```
 > plot(aggregate(AirPassengers,FUN=mean))
 ```
 <p style='text-align:center'><img src='https://github.com/xinyix/ARIMA/blob/master/aggregate_by_year.jpg?raw=true'></p>

We can also pull out some details of the seasonal effects using boxplot across a single cycle (12 month in this case)
```
> boxplot(AirPassengers~cycle(AirPassengers))
```
 <p style='text-align:center'><img src='https://github.com/xinyix/ARIMA/blob/master/boxplot_for_seasonality.jpg?raw=true'></p>
 
 Some important inferences we can make of the above exploration are
 1. The year on year trend shows the number of passengers has been increasing
 2. The mean and variance of July and August is much higher then the other months
 3. Although the mean levels are quite difference across the months, their difference in variance are small, thus there is strong seasonal effects with a period of 12 month

### Stationarize the Series
Since there is a trend and seasonality in our data, we want to address these two issues before fitting ARIMA models. We can use log transformation to get rid of unequal variances, then difference the series to take care of seasonality, so we propose
```
> adf.test(diff(log(AirPassengers)), alternative="stationary", k=0)

	Augmented Dickey-Fuller Test

data:  diff(log(AirPassengers))
Dickey-Fuller = -9.6003, Lag order = 0, p-value = 0.01
alternative hypothesis: stationary
```
The p-value is much smaller than 5%, so we reject the null hypothesis and conclude the resulting series is stationary. (Note that here we only took the first difference, which turns out to be sufficient)

### Try Various ARIMA Models
Now we try two approaches to determine the best ARIMA model for this data. First, take ARIMA(p, k, q) with p=0, 1, 2, k=0, 1, q=0, 1, 2 and compare their SSE. The one with smallest SSE is the best model. We start with the class of AR models
```
## partition the dataset, testing set is the last year
train <- window(AirPassengers, 1949, c(1959, 12))
test <- window(AirPassengers, 1960, c(1960, 12))

## create a matrix to hold all the SSE generated by different models we want to consider
SSE <- matrix(NA, nrow=21, ncol=2)
SSE[, 1] <- c('AR1', 'AR2', 'MA1', 'MA2', 'ARMA10', 'ARMA20', 'ARMA01', 'ARMA11', 'ARMA21', 'ARMA02', 'ARMA12', 'ARMA22', 'ARIMA010', 'ARIMA110', 'ARIMA210', 'ARIMA011', 'ARIMA111', 'ARIMA211', 'ARIMA012', 'ARIMA112', 'ARIMA212')

## AR models (AR(1), AR(2))
arima100 <- arima(train, order=c(1, 0, 0))
pred <- predict(arima100, n.ahead=12)
SSE[1, 2] <- sum((test - pred$pred)^2)

arima200 <- arima(train, order=c(2, 0, 0))
pred <- predict(arima200, n.ahead=12)
SSE[2, 2] <- sum((test - pred$pred)^2)

## MA models (MA(1), MA(2))
arima001 <- arima(train, order=c(0, 0, 1))
pred <- predict(arima001, n.ahead=12)
SSE[3, 2] <- sum((test - pred$pred)^2)

arima002 <- arima(train, order=c(0, 0, 2))
pred <- predict(arima002, n.ahead=12)
SSE[4, 2] <- sum((test - pred$pred)^2)

## ARMA models (ARMA(10), ARMA(20), ARMA(01), ARMA(11), ARMA(21), ARMA(02), ARMA(12), ARMA(22))
arima100 <- arima(train, order=c(1, 0, 0))
pred <- predict(arima100, n.ahead=12)
SSE[5, 2] <- sum((test - pred$pred)^2)

arima200 <- arima(train, order=c(2, 0, 0))
pred <- predict(arima200, n.ahead=12)
SSE[6, 2] <- sum((test - pred$pred)^2)

arima001 <- arima(train, order=c(0, 0, 1))
pred <- predict(arima001, n.ahead=12)
SSE[7, 2] <- sum((test - pred$pred)^2)

arima101 <- arima(train, order=c(1, 0, 1))
pred <- predict(arima101, n.ahead=12)
SSE[8, 2] <- sum((test - pred$pred)^2)

arima201 <- arima(train, order=c(2, 0, 1))
pred <- predict(arima201, n.ahead=12)
SSE[9, 2] <- sum((test - pred$pred)^2)

arima002 <- arima(train, order=c(0, 0, 2))
pred <- predict(arima002, n.ahead=12)
SSE[10, 2] <- sum((test - pred$pred)^2)

arima102 <- arima(train, order=c(1, 0, 2))
pred <- predict(arima102, n.ahead=12)
SSE[11, 2] <- sum((test - pred$pred)^2)

arima202 <- arima(train, order=c(2, 0, 2))
pred <- predict(arima202, n.ahead=12)
SSE[12, 2] <- sum((test - pred$pred)^2)

## ARIMA d=1 models (ARIMA(010), ARIMA(110), ARIMA(210), ARIMA(011), ARIMA(111), ARIMA(211), ARIMA(012), ARIMA(112), ARIMA(212))
arima010 <- arima(train, order=c(0, 1, 0))
pred <- predict(arima010, n.ahead=12)
SSE[13, 2] <- sum((test - pred$pred)^2)

arima110 <- arima(train, order=c(1, 1, 0))
pred <- predict(arima110, n.ahead=12)
SSE[14, 2] <- sum((test - pred$pred)^2)

arima210 <- arima(train, order=c(2, 1, 0))
pred <- predict(arima210, n.ahead=12)
SSE[15, 2] <- sum((test - pred$pred)^2)

arima011 <- arima(train, order=c(0, 1, 1))
pred <- predict(arima011, n.ahead=12)
SSE[16, 2] <- sum((test - pred$pred)^2)

arima111 <- arima(train, order=c(1, 1, 1))
pred <- predict(arima111, n.ahead=12)
SSE[17, 2] <- sum((test - pred$pred)^2)

arima211 <- arima(train, order=c(2, 1, 1))
pred <- predict(arima211, n.ahead=12)
SSE[18, 2] <- sum((test - pred$pred)^2)

arima012 <- arima(train, order=c(0, 1, 2))
pred <- predict(arima012, n.ahead=12)
SSE[19, 2] <- sum((test - pred$pred)^2)

arima112 <- arima(train, order=c(1, 1, 2))
pred <- predict(arima112, n.ahead=12)
SSE[20, 2] <- sum((test - pred$pred)^2)

arima212 <- arima(train, order=c(2, 1, 2))
pred <- predict(arima212, n.ahead=12)
SSE[21, 2] <- sum((test - pred$pred)^2)
```
The results, illustrated in a simple table is shown below
```
> SSE
      [,1]       [,2]              
 [1,] "AR1"      "200880.936776934"
 [2,] "AR2"      "233731.788404118"
 [3,] "MA1"      "591447.379622378"
 [4,] "MA2"      "577080.641974049"
 [5,] "ARMA10"   "200880.936776934"
 [6,] "ARMA20"   "233731.788404118"
 [7,] "ARMA01"   "591447.379622378"
 [8,] "ARMA11"   "211740.452454207"
 [9,] "ARMA21"   "188343.702461784"
[10,] "ARMA02"   "577080.641974049"
[11,] "ARMA12"   "166218.625598066"
[12,] "ARMA22"   "195306.686046089"
[13,] "ARIMA010" "127250"          
[14,] "ARIMA110" "100710.919754454"
[15,] "ARIMA210" "100470.779590424"
[16,] "ARIMA011" "96471.1949260153"
[17,] "ARIMA111" "99873.6755687539"
[18,] "ARIMA211" "92681.76312804"  
[19,] "ARIMA012" "99154.7393700196"
[20,] "ARIMA112" "99379.05925882"  
[21,] "ARIMA212" "102206.794202776"
```
From the table we can see our candidates are the models that generates small SSE, that is ARIMA(0, 1, 1) or ARIMA(2, 1, 1). To check this result, we look at the ACF and PACF of the stationarized series
```
> acf(log(AirPassengers))
> acf(diff(log(AirPassengers)))
> pacf(diff(log(AirPassengers)))
```
 <p style='text-align:center'><img src='https://github.com/xinyix/ARIMA/blob/master/acf_log.jpg?raw=true'></p>
ACF of the original series (log transformed) shows a slowly decaying pattern, this indicates the series is non-stationary. To get rid of the non-stationary, we take the first difference of the series and plot its ACF and PACF again
 <p style='text-align:center'><img src='https://github.com/xinyix/ARIMA/blob/master/acf_1d.jpg?raw=true'></p>
 <p style='text-align:center'><img src='https://github.com/xinyix/ARIMA/blob/master/pacf_1d.jpg?raw=true'></p>
The ACF oscilates around 0, indicating the first difference to be stationary (consistent with the Dickey-Fuller test result earlier). In addition, it truncates after the first lag, so p should be 0, while the value of q should be 1 or 2. This choice is consistent with our findings earlier. Finally, we go with ARIMA(0, 1, 1) since it's a simpler representation.

### Forecasts
We use ARIMA(0, 1, 1) to forecast the next 10 years
```
> (fit <- arima(log(AirPassengers), c(0, 1, 1),seasonal = list(order = c(0, 1, 1), period = 12)))

Call:
arima(x = log(AirPassengers), order = c(0, 1, 1), seasonal = list(order = c(0, 
    1, 1), period = 12))

Coefficients:
          ma1     sma1
      -0.4018  -0.5569
s.e.   0.0896   0.0731

sigma^2 estimated as 0.001348:  log likelihood = 244.7,  aic = -483.4
> pred <- predict(fit, n.ahead = 10*12)
> ts.plot(AirPassengers,2.718^pred$pred, log = "y", lty = c(1,3))
```
 <p style='text-align:center'><img src='https://github.com/xinyix/ARIMA/blob/master/predict.jpg?raw=true'></p>
